{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcUXGA3hsxdZ"
      },
      "source": [
        "# Text-as-Data Coursework Introduction\n",
        "\n",
        "The TaD coding coursework aims to assess your abilities to perform text processing techniques as applied to a multi-class classification problem.\n",
        "\n",
        "Your work will be submitted through a Moodle quiz. For each question, you should submit your text answer (providing the required information) separately from your code.\n",
        "\n",
        "## The Task\n",
        "\n",
        "A museum records organisation has a large set of records that need to be assigned to one of five institutions (in the table below). They have provided a small set of data to be used as the training, validation, and test set. Our goal is to build a classifier that could assign an unseen record to the correct institution.\n",
        "\n",
        "| Class Index | Institution                                |\n",
        "|-------------|--------------------------------------------|\n",
        "| 0           | National Maritime Museum                  |\n",
        "| 1           | National Railway Museum                   |\n",
        "| 2           | Royal Botanic Gardens, Kew                |\n",
        "| 3           | Royal College of Physicians of London     |\n",
        "| 4           | Shakespeare Birthplace Trust              |\n",
        "\n",
        "The dataset can be downloaded with the link: [Download Dataset](https://tinyurl.com/tadarchives)  \n",
        "\n",
        "## Generative AI Usage Policy\n",
        "\n",
        "You are free to use generative AI in any way during this assessed exercise. Please note your usage in the final question. Material from this coursework may appear on the final exam.\n",
        "\n",
        "## Questions\n",
        "\n",
        "### Q1: Training Data Cleaning [9 marks]\n",
        "\n",
        "Download and load the dataset. There are some issues with the training split of the data that would stop it from being used to train a classifier. Report all issues and how you fixed them.\n",
        "\n",
        "---\n",
        "\n",
        "### Q2: Exploration [5 marks]\n",
        "\n",
        "Once the training set has been fixed, report the following:\n",
        "\n",
        "- The sample counts for the training, validation, and test sets\n",
        "- The percentage splits for training, validation, and test sets\n",
        "- The minimum and maximum length (in characters) of the texts, reported separately for the training, validation, and test sets\n",
        "- The most frequent five tokens in each class (after tokenizing with `text_pipeline_spacy` from Lab 2)\n",
        "\n",
        "---\n",
        "\n",
        "### Q3: Prompting with a Large Language Model [10 marks]\n",
        "\n",
        "A colleague has tried prompting a large language model (Llama-3.1-8B-Instruct) to classify each of the records in the training set. They evaluated three different prompt templates and saved the results to the provided files.\n",
        "\n",
        "Calculate the following for each prompt template:\n",
        "\n",
        "- Accuracy\n",
        "- Macro precision\n",
        "- Macro recall\n",
        "- Macro F1\n",
        "\n",
        "Comment on the results. Consider any invalid output from the LLM as predicting a sixth hypothetical class.\n",
        "\n",
        "---\n",
        "\n",
        "### Q4: Fine-tune a Transformer [10 marks]\n",
        "\n",
        "Fine-tune a `bert-base-uncased` transformer model on the model using the training set. You should use an `AutoModelForSequenceClassification` and the HuggingFace `Trainer`. Use the following hyperparameters:\n",
        "\n",
        "- **Epochs:** 8\n",
        "- **Learning rate:** 5e-5\n",
        "- **Batch size:** 8\n",
        "\n",
        "Evaluate on the validation set. Report the following:\n",
        "\n",
        "- Per-class precision, recall, and F1 score\n",
        "- Accuracy\n",
        "- Macro precision\n",
        "- Macro recall\n",
        "- Macro F1 score\n",
        "\n",
        "Don’t be surprised with poor performance (see the next question).\n",
        "\n",
        "---\n",
        "\n",
        "### Q5: A Problem with the Validation Set [5 marks]\n",
        "\n",
        "There is an issue with the validation set which causes poor performance.\n",
        "\n",
        "- Provide the confusion matrix.\n",
        "- Describe the problem, how you identified it, and how you fixed it.\n",
        "\n",
        "---\n",
        "\n",
        "### Q6: Hyperparameter Tuning [12 marks]\n",
        "\n",
        "Train and evaluate several fine-tuned transformer models using the corrected training and validation sets. Try the four base models listed below. Use the following hyperparameters:\n",
        "\n",
        "- **Epochs:** 8\n",
        "- **Learning rate:** 5e-5\n",
        "- **Batch size:** 8\n",
        "\n",
        "Base models to try:\n",
        "\n",
        "- `bert-base-uncased`\n",
        "- `roberta-base`\n",
        "- `distilbert-base-uncased`\n",
        "- `microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract`\n",
        "\n",
        "We want the best model found during the training process for each base model and to save it for the final analysis. If the model after 3 epochs is the best performing on the validation set (by macro-F1), we want to keep that.\n",
        "\n",
        "You should investigate the `load_best_model_at_end` parameter for the `Trainer` (which does require other parameters).\n",
        "\n",
        "Evaluate each fine-tuned model on the validation set. Report the following:\n",
        "\n",
        "- Per-class precision, recall, and F1 score\n",
        "- Accuracy\n",
        "- Macro precision\n",
        "- Macro recall\n",
        "- Macro F1 score\n",
        "\n",
        "Comment on the performance of each model.\n",
        "\n",
        "---\n",
        "\n",
        "### Q7: Final Evaluation and Deployment [6 marks]\n",
        "\n",
        "Load the best model (based on macro-F1 on the validation set) that was saved in the previous question using a `text-classification` pipeline. Evaluate the best of the four fine-tuned models on the testing set.\n",
        "\n",
        "- State which model you used.\n",
        "- Report the per-class precision, recall, and F1 score.\n",
        "- Report accuracy, macro precision, macro recall, and macro F1 score.\n",
        "- Comment on the performance and discuss whether the quality is high enough to be deployed for the client.\n",
        "\n",
        "---\n",
        "\n",
        "### Q8: Generative AI Usage [1 mark]\n",
        "\n",
        "Report on whether and how you used generative AI in this assignment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2Kv21Jas0UI",
        "outputId": "28ae2b19-3977-4f7c-deb7-2a59d6b2c08a"
      },
      "outputs": [],
      "source": [
        "# !pip install fuzzywuzzy python-Levenshtein transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfXNteBzs3gG",
        "outputId": "815a98fa-8ff9-4d32-ac09-a4bbe297e291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using path: ./\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Set the path based on the environment\n",
        "if IN_COLAB:\n",
        "    drive.mount('/content/drive')\n",
        "    path = '/content/drive/MyDrive/Projects/TasD/'\n",
        "else:\n",
        "    # Adjust the local path to your project folder\n",
        "    path = os.path.expanduser('./')\n",
        "\n",
        "# Ensure the path exists\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "print(f\"Using path: {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vm9Hh2-0Yf2",
        "outputId": "702cc0ec-df4a-417b-9689-65504edcaf1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All required files are present.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Define the required filenames\n",
        "required_files = [\n",
        "    \"dataset.json\",\n",
        "    \"llm_prompt_template_1.json\",\n",
        "    \"llm_prompt_template_2.json\",\n",
        "    \"llm_prompt_template_3.json\"\n",
        "]\n",
        "\n",
        "# Check for missing files\n",
        "missing_files = [file for file in required_files if not os.path.exists(os.path.join(path, file))]\n",
        "\n",
        "# Download and extract if any file is missing\n",
        "if missing_files:\n",
        "    print(f\"Missing files: {', '.join(missing_files)}\")\n",
        "    print(\"Downloading and extracting the dataset...\")\n",
        "\n",
        "    # URL to the dataset\n",
        "    url = \"https://tinyurl.com/tadarchives\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Extract ZIP from response content\n",
        "        with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
        "            zip_ref.extractall(path)\n",
        "        print(\"Download and extraction complete.\")\n",
        "    else:\n",
        "        print(\"Failed to download the dataset. Please check the URL or your internet connection.\")\n",
        "else:\n",
        "    print(\"All required files are present.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KG6cayXMsxdf"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Mm9vl4R4sxdh"
      },
      "outputs": [],
      "source": [
        "with open(path+\"dataset.json\", \"r\") as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GnluPKGNsxdh"
      },
      "outputs": [],
      "source": [
        "# Standardized keys\n",
        "CONTENT_KEYS = [\"text\", \"description\", \"content\"]\n",
        "LABEL_KEYS = [\"label\", \"labl\",\"key\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut-B8okbsxdi"
      },
      "source": [
        "### Q1: Training Data Cleaning [9 marks]\n",
        "\n",
        "Download and load the dataset. There are some issues with the training split of the data that would stop it from being used to train a classifier. Report all issues and how you fixed them.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ_1G7Tpsxdi"
      },
      "source": [
        "## **Data Cleaning Process**  \n",
        "\n",
        "### **Objective:**  \n",
        "The goal of the `clean_entry` function is to clean and standardize dataset entries by:  \n",
        "- Consolidating multiple alternative keys for **content** (`text`, `description`, `content`) into a single standardized `content` key.  \n",
        "- Consolidating multiple alternative keys for **labels** (`label`, `labl`) into a standardized `label` key.  \n",
        "- Ensuring labels are converted to **strings** if they were stored as numbers.  \n",
        "- Checking for **unexpected keys** — any keys not related to `id`, `content`, or `label` — and issuing warnings if such keys are found.  \n",
        "- **Removing entries** where `content` or `label` is missing.  \n",
        "\n",
        "### **Key Steps:**  \n",
        "\n",
        "1. **Checking for unexpected keys:**  \n",
        "   - The function defines the allowed keys:  \n",
        "     - `id`  \n",
        "     - The standardized `content` and `label` keys  \n",
        "     - All possible content keys (`text`, `description`, `content`)  \n",
        "     - All possible label keys (`label`, `labl`)  \n",
        "   - Any additional keys are flagged as unexpected, and a **warning** is printed, but they are **not removed** from the entry.  \n",
        "\n",
        "2. **Standardizing the content key:**  \n",
        "   - The function looks for the first available content key (`text`, `description`, or `content`).  \n",
        "   - If none are found, `content` is set to `None`.  \n",
        "   - **Entries with null content are removed.**  \n",
        "\n",
        "3. **Standardizing the label key:**  \n",
        "   - The function looks for the first available label key (`label`, `labl`).  \n",
        "   - If the label is a number, it is **converted to a string**.  \n",
        "   - If the label is `null`, it remains `None`.  \n",
        "   - **Entries with null labels are removed.**  \n",
        "\n",
        "4. **Fuzzy Matching for Label Standardization:**  \n",
        "   - The function compares the extracted label against a predefined list of valid labels.  \n",
        "   - If a close enough match (score ≥ 80%) is found, the label is replaced with the standardized version.  \n",
        "   - Otherwise, a warning is issued, and the original label is kept.  \n",
        "\n",
        "5. **Removing Invalid Entries:**  \n",
        "   - If an entry has `None` for either `content` or `label`, it is removed from the dataset.  \n",
        "\n",
        "6. **Returning Cleaned Entries:**  \n",
        "   - A cleaned entry containing only `id`, `content`, and `label` is returned.  \n",
        "   - Any errors during processing are caught and logged, and `None` is returned for problematic entries.   \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oghkwDRdsxdj"
      },
      "outputs": [],
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "\n",
        "def clean_entry(entry):\n",
        "    \"\"\"Cleans a single record by standardizing keys, handling missing values, and checking for unexpected keys.\"\"\"\n",
        "    valid_labels = [\n",
        "        \"National Maritime Museum\",\n",
        "        \"Shakespeare Birthplace Trust\",\n",
        "        \"National Railway Museum\",\n",
        "        \"Royal Botanic Gardens, Kew\",\n",
        "        \"Royal College of Physicians of London\"\n",
        "    ]\n",
        "    try:\n",
        "        # Allowed keys for a valid entry\n",
        "        allowed_keys = set([\"id\", \"content\", \"label\"] + CONTENT_KEYS + LABEL_KEYS)\n",
        "\n",
        "        # Check for unexpected keys BEFORE cleaning\n",
        "        unexpected_keys = set(entry.keys()) - allowed_keys\n",
        "        if unexpected_keys:\n",
        "            print(f\"Warning: Unexpected keys found {unexpected_keys} in entry with id {entry.get('id')}\")\n",
        "\n",
        "        # Standardize content key\n",
        "        entry[\"content\"] = next(\n",
        "            (\n",
        "                entry[key]\n",
        "                for key in CONTENT_KEYS\n",
        "                if key in entry and isinstance(entry[key], str) and entry[key] is not None\n",
        "            ),\n",
        "            None,\n",
        "        )\n",
        "\n",
        "        # Standardize label key\n",
        "        entry[\"label\"] = next(\n",
        "            (\n",
        "                entry[key]\n",
        "                for key in LABEL_KEYS\n",
        "                if key in entry and isinstance(entry[key], str) and entry[key]\n",
        "                is not None\n",
        "            ),\n",
        "            None,\n",
        "        )\n",
        "\n",
        "        if entry[\"content\"] is None or entry[\"label\"] is None:\n",
        "            print(\n",
        "                f\"Removing entry with id {entry.get('id')} due to invalid content or label\"\n",
        "            )\n",
        "            return None\n",
        "\n",
        "        raw_label = entry[\"label\"]\n",
        "        matched_label = None\n",
        "        highest_score = 0\n",
        "\n",
        "        for valid_label in valid_labels:\n",
        "            score = fuzz.ratio(raw_label.strip(), valid_label.strip())\n",
        "            if score > highest_score:\n",
        "                highest_score = score\n",
        "                matched_label = valid_label\n",
        "\n",
        "        # If the highest score exceeds a threshold (e.g., 80%), accept the matched label\n",
        "        if highest_score >= 80:  # Adjust the threshold as needed\n",
        "            entry[\"label\"] = matched_label\n",
        "        else:\n",
        "            print(\n",
        "                f\"Warning: Label '{raw_label}' did not match any valid label well. Keeping original.\"\n",
        "            )\n",
        "\n",
        "        cleaned_entry = {\n",
        "            \"id\": entry.get(\"id\"),\n",
        "            \"label\": entry[\"label\"],\n",
        "            \"content\": entry[\"content\"],\n",
        "        }\n",
        "\n",
        "        return cleaned_entry\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing entry with id {entry.get('id')}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQcau_1Osxdj",
        "outputId": "869b38da-8e8b-4862-b9f4-9667b2cbb968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing entry with id 08838524-6d11-729c-39d8-fa4d712b7476 due to invalid content or label\n",
            "Removing entry with id 7ab43067-856c-5cee-8574-ac59a24de44b due to invalid content or label\n",
            "Removing entry with id 598e3e51-2390-addf-4363-09503488d4a5 due to invalid content or label\n",
            "Removing entry with id 4cbde7c0-1a7a-ef93-5720-a4b6b5f9c49f due to invalid content or label\n",
            "Removing entry with id b2e99517-4fdc-9e57-4557-580418e61c82 due to invalid content or label\n",
            "Removing entry with id 7148fce2-2315-1826-2615-05abd9b0b806 due to invalid content or label\n",
            "Removing entry with id d438b1f5-6dfa-24bc-7f0e-fc0161ee7be8 due to invalid content or label\n",
            "Removing entry with id b7a9494b-2738-ec6a-e5b3-523d2cb27b74 due to invalid content or label\n",
            "Removing entry with id 26dc19c0-9a55-2cae-20f9-a782a56d5c76 due to invalid content or label\n",
            "Removing entry with id ccd1b1b5-e044-98ca-d21e-7315c1e702f7 due to invalid content or label\n",
            "Removing entry with id 2e596900-d420-23e4-f5d1-c56cfc0b2352 due to invalid content or label\n",
            "Removing entry with id 24f77ce6-cd59-ff16-4a2e-a997dac08c40 due to invalid content or label\n",
            "Removing entry with id bfdcb596-3ec9-8f6e-7817-9eefcaf3dc84 due to invalid content or label\n",
            "Removing entry with id 59287772-bb77-05fe-9e17-5048ff184ae0 due to invalid content or label\n",
            "Removing entry with id af32a140-f84c-4f57-3304-97a963af96d4 due to invalid content or label\n",
            "Removing entry with id 3ed4e89a-15e0-acda-b40d-ab69bf501b9b due to invalid content or label\n",
            "Removing entry with id 05385824-cdf1-fc9e-b188-d99a3033bc55 due to invalid content or label\n",
            "Removing entry with id 3b534d3f-c3c6-c448-47a3-c8759e41a2fb due to invalid content or label\n",
            "Removing entry with id 62e52d58-cb89-4b02-c88f-d3ba8f5d4064 due to invalid content or label\n",
            "Removing entry with id b5eb8062-0b9b-ea2e-819b-c08261437877 due to invalid content or label\n",
            "Removing entry with id 7417f267-44fe-6042-39b6-1bc306c57a68 due to invalid content or label\n",
            "Removing entry with id 45576fc8-5ed6-e95a-f39a-1b9b6662a463 due to invalid content or label\n",
            "Removing entry with id 8adc2771-5f7e-e7c0-b3f1-9d68fa70dfc7 due to invalid content or label\n",
            "Removing entry with id cc664389-f352-c4b7-4bf4-a6afb7df6b82 due to invalid content or label\n",
            "Removing entry with id c575e068-7984-6662-3996-bd5d50031f46 due to invalid content or label\n",
            "Removing entry with id 6948eca2-243d-fc86-0f72-82d88387d292 due to invalid content or label\n",
            "Removing entry with id 6fc38b6f-f15c-2b56-717f-ff2f0e5bacf6 due to invalid content or label\n",
            "Removing entry with id 18849b8e-4167-5c5b-ddd3-41f3870993f8 due to invalid content or label\n",
            "Removing entry with id 9ba681e1-6a7b-167b-1471-b93094f02b31 due to invalid content or label\n",
            "Removing entry with id d319ef97-d4d0-1dd4-953a-8b70553512dc due to invalid content or label\n",
            "Removing entry with id df5b62b7-160b-79de-c7c8-499b0f968051 due to invalid content or label\n",
            "Removing entry with id 3c45e5c8-d7f7-6df7-804a-e96edc3b9f46 due to invalid content or label\n",
            "Removing entry with id d78a60b5-dfe4-8c4b-eeb3-6c2bf908f852 due to invalid content or label\n",
            "Removing entry with id aa985a46-605f-2be7-12ba-9204d8d240e4 due to invalid content or label\n",
            "Removing entry with id ba42f9ab-2d6f-50f0-8ef7-11235c19ce0a due to invalid content or label\n",
            "Removing entry with id b5b64e72-03f9-9604-de2a-698b86950df1 due to invalid content or label\n",
            "Removing entry with id bbdb7be0-9350-5a44-dd05-4bcc14db948e due to invalid content or label\n",
            "Removing entry with id d488fb79-d878-49d1-9b73-c7e0da7da022 due to invalid content or label\n",
            "Removing entry with id 7ea11f7d-e177-b9b4-0725-8def231d2436 due to invalid content or label\n",
            "Removing entry with id dbfa3a2a-5db1-7bd5-db10-c16f6fdc7dc8 due to invalid content or label\n",
            "Removing entry with id 00b214c8-6ea8-bcaa-4c56-caf1a8d27358 due to invalid content or label\n",
            "Removing entry with id a7e10af0-dda1-6788-1d44-6c26f97da428 due to invalid content or label\n",
            "Removing entry with id ad041b44-ab00-48aa-ffed-c0601c088fc5 due to invalid content or label\n",
            "Removing entry with id 88234fde-57b9-7eb1-4aa8-3a7038b04cf3 due to invalid content or label\n",
            "Removing entry with id dc6b92b8-ccc0-2b7c-3828-54c3ea1b53e4 due to invalid content or label\n",
            "Removing entry with id 79a1d90b-a68f-b2d6-cef0-524de18df0ed due to invalid content or label\n",
            "Removing entry with id 0274d6d9-2407-25f7-3230-2b77dd0ff63f due to invalid content or label\n",
            "Removing entry with id 2779b708-6fe6-2792-9ce0-6abef7560e9e due to invalid content or label\n",
            "Removing entry with id eee96233-4ff1-fdcf-e1af-179fabb1c3a2 due to invalid content or label\n",
            "Removing entry with id 95a0d569-602a-5bdc-06ca-62eb08302d3d due to invalid content or label\n",
            "Removing entry with id 246aeb01-6686-57ab-9f4e-3a15f01446e5 due to invalid content or label\n",
            "Removing entry with id 1a81f93b-6256-ff41-2117-a23a1c4d7156 due to invalid content or label\n",
            "Removing entry with id 65d546cc-8ca7-22a2-a3c4-53127fa591b8 due to invalid content or label\n"
          ]
        }
      ],
      "source": [
        "data[\"train\"] = [\n",
        "    clean_entry(entry) for entry in data[\"train\"] if clean_entry(entry) is not None\n",
        "]\n",
        "data[\"val\"] = [\n",
        "    clean_entry(entry) for entry in data[\"val\"] if clean_entry(entry) is not None\n",
        "]\n",
        "data[\"test\"] = [\n",
        "    clean_entry(entry) for entry in data[\"test\"] if clean_entry(entry) is not None\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1Xl03kXsxdl",
        "outputId": "36de2adf-86bf-4933-b72d-5e3de1c7ac9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset cleaned and saved as cleaned_dataset.json\n"
          ]
        }
      ],
      "source": [
        "with open(path+\"cleaned_dataset.json\", \"w\") as file:\n",
        "    json.dump(data, file, indent=4)\n",
        "\n",
        "print(\"Dataset cleaned and saved as cleaned_dataset.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Rg36lTh-sxdm"
      },
      "outputs": [],
      "source": [
        "with open(path+\"cleaned_dataset.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "train_data = data[\"train\"]\n",
        "val_data = data[\"val\"]\n",
        "test_data = data[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8csms87Rsxdm"
      },
      "source": [
        "### Q2: Exploration [5 marks]\n",
        "\n",
        "Once the training set has been fixed, report the following:\n",
        "\n",
        "- The sample counts for the training, validation, and test sets\n",
        "- The percentage splits for training, validation, and test sets\n",
        "- The minimum and maximum length (in characters) of the texts, reported separately for the training, validation, and test sets\n",
        "- The most frequent five tokens in each class (after tokenizing with `text_pipeline_spacy` from Lab 2)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F4Ptennsxdn",
        "outputId": "e1dcda1d-e632-4110-83b5-31897277fde6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Counts:\n",
            "Training set: 102\n",
            "Validation set: 50\n",
            "Test set: 50\n",
            "\n",
            "Percentage Splits:\n",
            "Training set: 50.50%\n",
            "Validation set: 24.75%\n",
            "Test set: 24.75%\n"
          ]
        }
      ],
      "source": [
        "# Sample counts\n",
        "train_count = len(train_data)\n",
        "val_count = len(val_data)\n",
        "test_count = len(test_data)\n",
        "total_count = train_count + val_count + test_count\n",
        "\n",
        "# Percentage splits\n",
        "train_pct = (train_count / total_count) * 100\n",
        "val_pct = (val_count / total_count) * 100\n",
        "test_pct = (test_count / total_count) * 100\n",
        "\n",
        "# Display results\n",
        "print(\"Sample Counts:\")\n",
        "print(f\"Training set: {train_count}\")\n",
        "print(f\"Validation set: {val_count}\")\n",
        "print(f\"Test set: {test_count}\")\n",
        "\n",
        "print(\"\\nPercentage Splits:\")\n",
        "print(f\"Training set: {train_pct:.2f}%\")\n",
        "print(f\"Validation set: {val_pct:.2f}%\")\n",
        "print(f\"Test set: {test_pct:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giIEfXoFsxdn",
        "outputId": "73935ea3-130a-40bb-8d3b-4d5582276414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Text Lengths (in characters):\n",
            "Training set - Min: 163, Max: 2349\n",
            "Validation set - Min: 154, Max: 2794\n",
            "Test set - Min: 167, Max: 3479\n"
          ]
        }
      ],
      "source": [
        "# Function to calculate min/max lengths of content\n",
        "def get_text_lengths(data):\n",
        "    lengths = [len(entry[\"content\"]) for entry in data if entry[\"content\"]]\n",
        "    return min(lengths, default=0), max(lengths, default=0)\n",
        "\n",
        "\n",
        "# Get lengths for each set\n",
        "train_min, train_max = get_text_lengths(train_data)\n",
        "val_min, val_max = get_text_lengths(val_data)\n",
        "test_min, test_max = get_text_lengths(test_data)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nText Lengths (in characters):\")\n",
        "print(f\"Training set - Min: {train_min}, Max: {train_max}\")\n",
        "print(f\"Validation set - Min: {val_min}, Max: {val_max}\")\n",
        "print(f\"Test set - Min: {test_min}, Max: {test_max}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88EAgSHTsxdo",
        "outputId": "987c8d68-4772-4484-c86b-4cd81fbc41c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 5 Tokens Per Class (Training set):\n",
            "Class Royal Botanic Gardens, Kew: [('letter', 25), ('paper', 12), ('include', 9), ('ridley', 9), ('journal', 8)]\n",
            "Class Shakespeare Birthplace Trust: [('mr.', 10), ('account', 10), ('letter', 10), ('william', 7), ('bridges', 6)]\n",
            "Class National Railway Museum: [('2000', 108), ('7200', 108), ('756', 107), ('gb', 106), ('water', 23)]\n",
            "Class Royal College of Physicians of London: [('mr.', 17), ('seal', 15), ('plate', 14), ('college', 14), ('common', 13)]\n",
            "Class National Maritime Museum: [('sir', 10), ('henry', 8), ('john', 7), ('enclosure', 7), ('land', 6)]\n",
            "{'National Maritime Museum': '0', 'National Railway Museum': '1', 'Royal Botanic Gardens, Kew': '2', 'Royal College of Physicians of London': '3', 'Shakespeare Birthplace Trust': '4'}\n",
            "{'467eb98e-1866-708e-03cd-9ab9f8a7f4e5': '2', 'b78f8473-e487-b3cb-be65-4f854d97228b': '2', 'ea15bfc9-9499-e9e2-b3e4-654714a795a5': '4', '03556d52-3c1e-f36e-97bd-1a09c87ef949': '1', 'f94bc0a8-a0a4-c96f-e3a8-9e71050ff05a': '1', '96355c6a-8bf5-00af-3c97-e297c1967745': '1', 'd676f9c6-54b9-7651-4c8d-c47aa629ed13': '1', 'de0a045a-a9e7-4f22-86c3-e06f83bdfa9d': '2', '86813922-dc1b-66c4-d716-4d5d4f957427': '3', '6409e148-9829-1408-1054-d325e21d3827': '4', 'e58d9f87-db53-e059-89ed-1a827c9fb62d': '4', 'd3bfae85-1bfa-2fa2-7769-4490fe02405c': '0', 'c69f0c63-dd60-aa63-1e50-67c436eb2d47': '4', 'a764efee-e975-bf0a-fcc0-fd9a232de386': '2', 'ff396a5b-52bc-663a-7c38-a48e64fae9f8': '1', '51c50843-deb5-fca6-e288-06174b784fcd': '2', '2779b968-26d3-1365-600d-e851dfd39f7e': '3', 'e03008f5-c7a4-001e-3bfb-6cf9d8b3c742': '3', '11d6f4b9-de12-33d6-98a6-a6858dab43e9': '0', '48efca0b-cd73-9f09-3794-8def93afa8aa': '3', '323d05c3-4289-6ec2-d006-c543e5e5cbf0': '4', '2c3a33d9-8355-9e17-6851-c63400e17235': '1', 'fc77df02-3f69-5b20-0877-ea8a38e2c8d3': '3', '6a809084-2fdc-dd2a-ecc8-b326251156d1': '2', '2c7404af-65ac-a67f-1a08-cb51f8c7d7f0': '0', '6d65a530-e173-dc1f-556e-f898b8b6331a': '0', 'dc604341-7aa9-e42e-4fee-5a1442855936': '1', '219bcb06-bd4c-6a6d-eb9a-363b08c864fc': '4', '87499534-fde7-90d9-6a12-d1a3405a6617': '3', '4cbccb71-bea5-4829-9073-6271796653bf': '2', 'd6600342-1745-140f-cdaa-17ea421b92fc': '1', '4d706888-5265-4cd5-5009-9a8a97513403': '2', 'a4188a00-55e2-2ad7-24aa-4ae2f90a7661': '3', 'f9da110a-fe5e-6413-963d-cd6583787d78': '3', 'd0e2ff95-623e-0b6c-306d-c454d1864bd2': '3', '5242860f-7d6b-1244-6ba8-5cc1d4351a54': '1', '3d19eb06-3b50-576c-1d0f-67b042f3243d': '1', '266b28c5-dbd9-79fc-2dc0-07e65a33c890': '0', 'dbfbeffb-aee9-b6cf-37e8-64dcb72afd6f': '1', 'f2284f50-5452-ef64-e3c8-ae38fcdac7b5': '1', '52a87c5d-6740-57d4-7ae3-aa675e6ef55d': '0', 'f66bffdd-e725-f8d2-5672-0a48b7d9c4fb': '4', '26be998a-5cda-5ff2-a1d2-c48673eb3595': '1', 'c318fb75-5395-cef2-9d0e-b6f1828d640f': '1', '499e950f-18f1-b4bb-3790-5f845e2077b5': '0', '57ba542e-2559-307b-5a04-90d4dbc8f595': '2', '9385bd89-6cce-e759-4597-f5a64f62cd18': '2', '2216ca39-7e82-cd31-96e2-3696413415a0': '3', '11244863-9259-f4a3-62a9-d073ae7fbdc3': '1', '658f160d-0dfc-4637-04e4-400ecc611264': '4', '5bd0a864-7b07-f80e-1690-52f875c72350': '3', '602d8028-2039-7e59-ebfd-ca603158c76e': '4', 'dc6bc369-de74-90de-9947-24e2548a8ada': '2', '23ab4fb9-7092-5ca3-095a-b481f4d9edaa': '0', '54028170-b711-081f-e637-3dcac50b9891': '3', '6f691179-4455-0b9a-2948-63f894ea1d86': '3', '40183fe1-22b4-b511-62db-133f5c8f2e8c': '1', 'd4165b4d-4064-5978-acc1-10513b9d7412': '1', '958c91ac-6cdb-0712-7b89-34455d688974': '1', 'b990e183-f776-7cb6-c57c-6dd86bb7f8d1': '2', '173a2c84-7e60-28c5-31f9-ecafa12af0e0': '2', 'bfa6bfae-26ab-afb7-e4a6-7090c7921997': '1', '297c543c-443b-2a21-a0ab-4e03a2719c45': '3', '3388dc8d-bd5a-6e24-194c-55a57a36da3a': '0', '43a3c06c-125c-ecf3-42e7-155e2207d56a': '3', '4432b026-366a-f7a3-e514-cb9d5e2629c8': '2', 'b60a1e66-3757-a468-2b77-68cfa8cf33e5': '0', 'a719e5f3-d2ba-f8c8-842c-3ec1317cc5d9': '3', '6ef457de-8417-643c-6e03-dc6883486ef1': '3', '69184f40-070d-bb89-3f39-7145529b75c7': '3', '94474c55-5660-4706-63f3-9422a759fda0': '4', '966de176-4252-8f02-6d50-d7c28e617eea': '2', '776a38bf-dcd3-913d-3c79-cc4795375f17': '1', 'e5682318-7738-bd1f-b339-a0fa1ff247e5': '0', '194e39e8-6c63-a28d-2fff-96b6502b9af3': '2', '1569acd0-030c-7a99-870b-f3d518b52559': '2', '74dda342-3f80-25f3-59a3-f58de1537b35': '3', '58a560e8-1895-1552-62b9-80ec3aca2420': '0', '591dad1f-a198-8f5e-7c08-7d7a9352860a': '2', '89d53786-7e23-a928-f74d-003f5d7062aa': '0', 'e9f50cae-250e-a692-8e96-82bffd4c311e': '3', '7bdae410-a8f0-db6c-3780-7beba41262d5': '1', 'cbd9c8bb-55af-d00b-4ca7-ff629c8f6916': '0', '83a600e6-bcd4-c581-9ecf-00cd39eadc2c': '1', '2dcbf6be-60a4-2a0f-d83a-d01f68755a4d': '3', '66b76560-f567-ac8e-8ff2-b13c1bd9e661': '1', '0813d0b3-9f0a-aa92-ecdd-60ea4ff6a9a6': '3', '50145e24-0917-9434-f461-a42b0c1ce044': '2', 'ac5cf713-02fe-08f1-d34b-3e40f0272f11': '4', '77d27575-c238-73d6-d12a-b3c27f0582fb': '4', 'b5980c54-c033-9670-679b-04decb68df86': '4', 'eda86546-5f67-cca0-b033-45b5e1c9002b': '4', '62fd3c91-f6fb-2911-b956-63e1f4207492': '4', '2baf0cbc-5763-f2bb-7976-121cd3ff7a3d': '0', 'dfc34085-6d18-fec7-fdea-c97e5b6c8f03': '3', '2045355e-c13a-7ab4-a6a6-bf348a697b53': '2', '79c0baa7-4414-5f4d-45a5-ed63b26dafab': '2', '93c3426b-75c0-19aa-9cb8-5da9d055363d': '4', '32fb4129-3117-002c-cb96-2b65bb430b75': '4', '0696ec91-691f-3e27-f5c2-5628ccd88956': '0', '5911d963-3dae-5839-2443-960930f73aa3': '0', 'cdba4d41-7d2b-e7ea-5b15-77c7b7c1b6e0': '4'}\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "# Tokenization using text_pipeline_spacy\n",
        "def tokenize(text):\n",
        "    tokens = []\n",
        "    doc = nlp(text)\n",
        "    for t in doc:\n",
        "        if not t.is_stop and not t.is_punct and not t.is_space:\n",
        "            tokens.append(t.lemma_.lower())\n",
        "    return tokens\n",
        "\n",
        "# Get most frequent tokens per class\n",
        "def get_top_tokens_by_class(data, top_n=5):\n",
        "    class_tokens = {}\n",
        "\n",
        "    for entry in data:\n",
        "        label = entry[\"label\"]\n",
        "        tokens = tokenize(entry[\"content\"]) if entry[\"content\"] else []\n",
        "        if label not in class_tokens:\n",
        "            class_tokens[label] = Counter()\n",
        "        class_tokens[label].update(tokens)\n",
        "\n",
        "    # Get the top N tokens for each class\n",
        "    top_tokens = {\n",
        "        label: counter.most_common(top_n) for label, counter in class_tokens.items()\n",
        "    }\n",
        "    return top_tokens\n",
        "\n",
        "label_to_class_mapping = {\n",
        "    'National Maritime Museum': '0',\n",
        "    'National Railway Museum': '1',\n",
        "    'Royal Botanic Gardens, Kew': '2',\n",
        "    'Royal College of Physicians of London': '3',\n",
        "    'Shakespeare Birthplace Trust': '4'\n",
        "  }\n",
        "# Create label-to-class mapping\n",
        "def create_label_mapping(train_data):\n",
        "    return {\n",
        "        entry[\"id\"]: label_to_class_mapping.get(entry[\"label\"], \"5\")  # Default to \"5\" for unknown labels\n",
        "        for entry in train_data\n",
        "        if \"id\" in entry and \"label\" in entry\n",
        "    }\n",
        "\n",
        "# Get top tokens for each dataset\n",
        "train_top_tokens = get_top_tokens_by_class(train_data)\n",
        "val_top_tokens = get_top_tokens_by_class(val_data)\n",
        "test_top_tokens = get_top_tokens_by_class(test_data)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nTop 5 Tokens Per Class (Training set):\")\n",
        "for label, tokens in train_top_tokens.items():\n",
        "    print(f\"Class {label}: {tokens}\")\n",
        "\n",
        "label_mapping = create_label_mapping(train_data)\n",
        "\n",
        "\n",
        "print(label_to_class_mapping)\n",
        "print(label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2A31nQlisxdo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "valid_classes = {\"0\", \"1\", \"2\", \"3\", \"4\"}\n",
        "# Map LLM predictions to classes (invalid outputs go to class 5)\n",
        "def map_llm_prediction(next_token):\n",
        "    return int(next_token) if next_token in valid_classes else 5\n",
        "\n",
        "# Load LLM results and match them to true labels\n",
        "def load_llm_results(file_path, label_mapping):\n",
        "    with open(path + file_path, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    results = []\n",
        "    for entry in data:\n",
        "        entry_id = entry.get(\"id\")\n",
        "        true_label = int(label_mapping.get(entry_id, 5))\n",
        "        next_token = entry.get(\"next_token\", \"\").strip()\n",
        "        predicted_label = map_llm_prediction(next_token)\n",
        "        # print(f\"{entry_id}, next_token: {next_token}, (true_label:predicted_label): ({true_label}:{predicted_label})\")\n",
        "\n",
        "        results.append({\"true_label\": true_label, \"predicted_label\": predicted_label})\n",
        "\n",
        "    return results\n",
        "\n",
        "# Calculate accuracy, macro precision, recall, and F1 score\n",
        "def calculate_metrics(results):\n",
        "    # print(results)\n",
        "    true_labels = [result[\"true_label\"] for result in results]\n",
        "    predicted_labels = [result[\"predicted_label\"] for result in results]\n",
        "\n",
        "    # Accuracy\n",
        "    # accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "    # Precision, recall, and F1 for each class (0-5)\n",
        "    # precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    #     true_labels, predicted_labels, average=None, labels=[0, 1, 2, 3, 4, 5], zero_division=0\n",
        "    # )\n",
        "\n",
        "    valid_results = [(true, pred) for true, pred in zip(true_labels, predicted_labels) if true != 5 and pred != 5]\n",
        "    valid_true_labels = [true for true, _ in valid_results]\n",
        "    valid_predicted_labels = [pred for _, pred in valid_results]\n",
        "    # print(valid_results)\n",
        "    # print(valid_true_labels)\n",
        "    # print(valid_predicted_labels)\n",
        "\n",
        "    # Accuracy (only valid predictions)\n",
        "    accuracy = accuracy_score(valid_true_labels, valid_predicted_labels) if valid_true_labels else 0.0\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        true_labels, predicted_labels, average=\"macro\", labels=[0, 1, 2, 3, 4,5], zero_division=0\n",
        "    )\n",
        "\n",
        "    # Macro precision, recall, and F1\n",
        "    macro_precision = np.mean(precision)\n",
        "    macro_recall = np.mean(recall)\n",
        "    macro_f1 = np.mean(f1)\n",
        "\n",
        "    return {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Macro Precision\": macro_precision,\n",
        "        \"Macro Recall\": macro_recall,\n",
        "        \"Macro F1\": macro_f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1cew6wMsxdo",
        "outputId": "a9c8c9a0-cb21-4dc1-dc09-c06184748196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt 1 Metrics:\n",
            "Accuracy: 0.0\n",
            "Macro Precision: 0.05333333333333334\n",
            "Macro Recall: 0.16666666666666666\n",
            "Macro F1: 0.08080808080808081\n",
            "\n",
            "==================================================\n",
            "\n",
            "Prompt 2 Metrics:\n",
            "Accuracy: 0.7843137254901961\n",
            "Macro Precision: 0.44549604705801643\n",
            "Macro Recall: 0.6380025440127742\n",
            "Macro F1: 0.5087234738327103\n",
            "\n",
            "==================================================\n",
            "\n",
            "Prompt 3 Metrics:\n",
            "Accuracy: 0.7216494845360825\n",
            "Macro Precision: 0.4693225321301613\n",
            "Macro Recall: 0.5625067659914207\n",
            "Macro F1: 0.46729988965283087\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load predictions for each prompt template\n",
        "prompt1_results = load_llm_results(\"llm_prompt_template_1.json\",label_mapping)\n",
        "prompt2_results = load_llm_results(\"llm_prompt_template_2.json\",label_mapping)\n",
        "prompt3_results = load_llm_results(\"llm_prompt_template_3.json\",label_mapping)\n",
        "\n",
        "# Evaluate predictions\n",
        "prompt1_metrics = calculate_metrics(prompt1_results)\n",
        "prompt2_metrics = calculate_metrics(prompt2_results)\n",
        "prompt3_metrics = calculate_metrics(prompt3_results)\n",
        "\n",
        "# Display the results\n",
        "for i, metrics in enumerate([prompt1_metrics, prompt2_metrics, prompt3_metrics], 1):\n",
        "    print(f\"Prompt {i} Metrics:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value}\")\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwAXNDhasxdp",
        "outputId": "52bf616f-e326-4653-f984-3ccf2cf7d0b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", num_labels=6\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "518d7147b5ef4ce29cca50f427fbe5c0",
            "ecc7d7ec8b65459692904d26859d0a3a",
            "6f38bb97a35e4038b13ee3d731639cc8",
            "7324cc83872b4118a8572b7f979c75a9",
            "8e7105458491467188f82d4d36d5fa5a",
            "aa08798075ff4f27a6a44180ea253abb",
            "58cb8a3d4a41402b91fa2d67e80288d3",
            "3fb991decce54d28a27cee9c920ec0ff",
            "a0be0f89a2da46c5af2fc5a11eacba7f",
            "430b95c169cc474197dab7f4b651c1d8",
            "712c747798894d1b9c28ab6f63f18cfe",
            "76f1894e22a14b458a60affd4296bf5e",
            "3ae12402eff04c289f2de8f8b2cc41c6",
            "6953c898a5874a5c8e810aca25378ea0",
            "b02cb5698e7b427392af57d5c1eed004",
            "eb6fcfcc9d1342d68e2596086a47e370",
            "389918e85acc4a6382852ef2d59ae195",
            "16a12775c600499c937516ac70689c54",
            "a581e6c45e784d86b4230c5616de9561",
            "74ab238b6d1e4804b38533b03d116168",
            "61f9c7b08fa245dea5ca06a778e7e5b5",
            "03373b86e4184a728befbef79a0d9e56"
          ]
        },
        "id": "IFJrldelsxdp",
        "outputId": "95d18dad-8864-4ec6-cf21-b02558fb395f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f4b2f6135714abdb4bcfdb31e6e992a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/102 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ee7f183f838469580d1d39008ae0eaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokenized_inputs = tokenizer(examples['content'], padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Ensure labels are correctly batched as lists\n",
        "    tokenized_inputs[\"labels\"] = examples['label']  # Pass the whole batch of labels directly\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"content\": [x[\"content\"] for x in train_data],\n",
        "    \"label\": [int(label_to_class_mapping.get(x[\"label\"], 5)) for x in train_data]  # Default to 5 if label not found\n",
        "})\n",
        "\n",
        "val_dataset = Dataset.from_dict({\n",
        "    \"content\": [x[\"content\"] for x in val_data],\n",
        "    \"label\": [int(label_to_class_mapping.get(x[\"label\"], 5)) for x in val_data]\n",
        "})\n",
        "\n",
        "# Tokenize datasets\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3ONAG2axaN23"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# Load each metric individually\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    precision = precision_metric.compute(\n",
        "        predictions=predictions, references=labels, average=\"macro\",\n",
        "    )[\"precision\"]\n",
        "    recall = recall_metric.compute(\n",
        "        predictions=predictions, references=labels, average=\"macro\",\n",
        "    )[\"recall\"]\n",
        "    f1 = f1_metric.compute(\n",
        "        predictions=predictions, references=labels, average=\"macro\",\n",
        "    )[\"f1\"]\n",
        "    accuracy = accuracy_metric.compute(\n",
        "        predictions=predictions, references=labels\n",
        "    )[\"accuracy\"]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"macro_precision\": precision,\n",
        "        \"macro_recall\": recall,\n",
        "        \"macro_f1\": f1,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtDnol3Qa2gd",
        "outputId": "b40a9fac-fce6-4d41-bf91-357c688ab506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Model is on device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model is on device: {model.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tbIawNzZsxdp"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=8,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=200,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_macro_f1\",\n",
        "    no_cuda=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "SRkLwdGksxdq",
        "outputId": "bbad5761-34b5-4d9e-e25a-691a12d60ed8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  4/104 01:17 < 1:04:34, 0.03 it/s, Epoch 0.23/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# from google.colab import userdata\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# wandb_api_key = userdata.get('WANDB_API_KEY')\u001b[39;00m\n\u001b[32m      4\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mWANDB_API_KEY\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mfe6f46e7acd9db3ef7cd76e9bf2abf44993ed01c\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/transformers/trainer.py:2241\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/transformers/trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/transformers/trainer.py:3698\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3695\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3697\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3698\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3700\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3701\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3702\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3703\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3704\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/transformers/trainer.py:3759\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3757\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3758\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3759\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3760\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3761\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3762\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1673\u001b[39m, in \u001b[36mBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1665\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1666\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1667\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1668\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1669\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1670\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1671\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1673\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1680\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1681\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1683\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1685\u001b[39m pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1687\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1140\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1154\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1155\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    574\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    575\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    583\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    584\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    592\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    594\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    506\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    507\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    514\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    525\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:395\u001b[39m, in \u001b[36mBertSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().forward(\n\u001b[32m    384\u001b[39m         hidden_states,\n\u001b[32m    385\u001b[39m         attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m         output_attentions,\n\u001b[32m    391\u001b[39m     )\n\u001b[32m    393\u001b[39m bsz, tgt_len, _ = hidden_states.size()\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m query_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[38;5;66;03m# mask needs to be such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[32m    399\u001b[39m is_cross_attention = encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/media/Documents/GLA/Text as Data/Coursework 1/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "# from google.colab import userdata\n",
        "# wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = \"fe6f46e7acd9db3ef7cd76e9bf2abf44993ed01c\"\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvjcI8Cgsxdq"
      },
      "outputs": [],
      "source": [
        "results = trainer.evaluate()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-wrSVHre_8a"
      },
      "source": [
        "## Q5: A problem with the validation set [5 marks]\n",
        "\n",
        "There is an issue with the validation set which causes poor performance. Provide the confusion matrix. Describe the\n",
        "problem, how you identified it and how you fixed it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVDStWFecEL0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "# Get predictions and true labels from the validation dataset\n",
        "predictions = trainer.predict(val_dataset)\n",
        "\n",
        "# Extract predicted labels (the model's predictions)\n",
        "predicted_labels = predictions.predictions.argmax(axis=-1)\n",
        "\n",
        "# True labels from the validation dataset\n",
        "true_labels = predictions.label_ids\n",
        "\n",
        "# Assuming you have predictions and true labels in the 'predictions' and 'true_labels' variables\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Display the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPFmCE4mgNDu"
      },
      "source": [
        "## Q6: Hyperparameter tuning [12 marks]\n",
        "\n",
        "Train and evaluate several fine-tuned transformer models using the corrected training and validation sets. Try the\n",
        "four base models listed below. Use 8 epochs, a learning_rate of 5e-5 and a batch size of 8.  Ideally, you would try\n",
        "different base_models/learning_rates/batch_sizes/etc, but we will limit this to evaluating four different base models\n",
        "and keep the remaining hyperparameters static.\n",
        "Base models to try:  'bert-base-uncased','roberta-base','distilbert-base-uncased', and\n",
        "'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract'  \n",
        "This time, we want the best model found during the training process for each base model and to save it for the final\n",
        "analysis. For example, if the model after 3 epochs is the best performing on the validation set (by macro-F1), we want\n",
        "to keep that. You should investigate the load_best_model_at_end parameter for the Trainer (which does require\n",
        "other parameters).\n",
        "Evaluate each fine-tuned model on the validation set. Report the per-class precision, recall and F1 score as well as the\n",
        "accuracy, macro precision, macro recall and macro F1 score. Comment on the performance of each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6jelAELfNlu"
      },
      "outputs": [],
      "source": [
        "def compute_metrics2(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(axis=-1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU9T6Yt0gWZ1"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Save outputs here\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=8,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=200,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,  # Load the best model based on evaluation metrics\n",
        "    metric_for_best_model=\"eval_macro_f1\",  # Select the best model based on macro F1 score\n",
        "    no_cuda=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHzmW0GQgXil"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(base_model_name, train_dataset, val_dataset, tokenizer):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=5)  # Change num_labels to match your dataset\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        processing_class=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    return model, eval_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsUIrJOsgY6a"
      },
      "outputs": [],
      "source": [
        "base_models = [\n",
        "    'bert-base-uncased',\n",
        "    'roberta-base',\n",
        "    'distilbert-base-uncased',\n",
        "    'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract'\n",
        "]\n",
        "\n",
        "# Assuming you have train_dataset, val_dataset, and tokenizer already defined\n",
        "best_models = {}\n",
        "eval_results = {}\n",
        "\n",
        "for model_name in base_models:\n",
        "    print(f\"Training and evaluating model: {model_name}\")\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    model, results = train_and_evaluate(model_name, train_dataset, val_dataset, tokenizer)\n",
        "\n",
        "    # Save the best model for each base model\n",
        "    best_models[model_name] = model\n",
        "    eval_results[model_name] = results\n",
        "\n",
        "    print(f\"Evaluation results for {model_name}: {results}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdkPjGKegaPK"
      },
      "outputs": [],
      "source": [
        "for model_name, results in eval_results.items():\n",
        "    print(f\"Evaluation results for {model_name}:\")\n",
        "    print(f\"  Accuracy: {results['eval_accuracy']:.4f}\")\n",
        "    print(f\"  Macro Precision: {results['eval_macro_precision']:.4f}\")\n",
        "    print(f\"  Macro Recall: {results['eval_macro_recall']:.4f}\")\n",
        "    print(f\"  Macro F1: {results['eval_macro_f1']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeBxw830giNr"
      },
      "source": [
        "## Q7: Final evaluation and deployment [6 marks]\n",
        "\n",
        "Load the best model (based on macro-F1 on the validation set) that was saved in the previous question using a\n",
        "‘text-classification’ pipeline. Evaluate the best of the four fine-tuned models on the testing set.  \n",
        "State which model you used and report the per-class precision, recall and F1 score as well as the accuracy, macro\n",
        "precision, macro recall and macro F1 score. Comment on the performance and discuss whether the quality is high\n",
        "enough to be deployed for the client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbqL-xVkgkGx"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the best model for the base model with the highest macro-F1 score\n",
        "best_model = best_models['roberta-base']  # Example: you should replace with the actual best model based on the previous analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaJ0agY4gmHp"
      },
      "outputs": [],
      "source": [
        "text_classifier = pipeline(\"text-classification\", model=best_model, tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpH_2idPgm_1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# Assuming test_dataset is defined with the necessary format\n",
        "true_labels = [example['label'] for example in test_dataset]\n",
        "predictions = []\n",
        "\n",
        "for example in test_dataset:\n",
        "    text = example['text']  # Assuming 'text' field contains the input text\n",
        "    pred = text_classifier(text)\n",
        "    predictions.append(pred[0]['label'])  # 'label' is the predicted class\n",
        "\n",
        "# Calculate metrics\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average=None)\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "\n",
        "# Calculate macro scores\n",
        "macro_precision = precision.mean()\n",
        "macro_recall = recall.mean()\n",
        "macro_f1 = f1.mean()\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Per-class Precision: {precision}\")\n",
        "print(f\"Per-class Recall: {recall}\")\n",
        "print(f\"Per-class F1: {f1}\")\n",
        "print(f\"Macro Precision: {macro_precision:.4f}\")\n",
        "print(f\"Macro Recall: {macro_recall:.4f}\")\n",
        "print(f\"Macro F1: {macro_f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03373b86e4184a728befbef79a0d9e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16a12775c600499c937516ac70689c54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "389918e85acc4a6382852ef2d59ae195": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ae12402eff04c289f2de8f8b2cc41c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_389918e85acc4a6382852ef2d59ae195",
            "placeholder": "​",
            "style": "IPY_MODEL_16a12775c600499c937516ac70689c54",
            "value": "Map: 100%"
          }
        },
        "3fb991decce54d28a27cee9c920ec0ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "430b95c169cc474197dab7f4b651c1d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "518d7147b5ef4ce29cca50f427fbe5c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ecc7d7ec8b65459692904d26859d0a3a",
              "IPY_MODEL_6f38bb97a35e4038b13ee3d731639cc8",
              "IPY_MODEL_7324cc83872b4118a8572b7f979c75a9"
            ],
            "layout": "IPY_MODEL_8e7105458491467188f82d4d36d5fa5a"
          }
        },
        "58cb8a3d4a41402b91fa2d67e80288d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61f9c7b08fa245dea5ca06a778e7e5b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6953c898a5874a5c8e810aca25378ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a581e6c45e784d86b4230c5616de9561",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74ab238b6d1e4804b38533b03d116168",
            "value": 50
          }
        },
        "6f38bb97a35e4038b13ee3d731639cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fb991decce54d28a27cee9c920ec0ff",
            "max": 102,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0be0f89a2da46c5af2fc5a11eacba7f",
            "value": 102
          }
        },
        "712c747798894d1b9c28ab6f63f18cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7324cc83872b4118a8572b7f979c75a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_430b95c169cc474197dab7f4b651c1d8",
            "placeholder": "​",
            "style": "IPY_MODEL_712c747798894d1b9c28ab6f63f18cfe",
            "value": " 102/102 [00:00&lt;00:00, 430.97 examples/s]"
          }
        },
        "74ab238b6d1e4804b38533b03d116168": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76f1894e22a14b458a60affd4296bf5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ae12402eff04c289f2de8f8b2cc41c6",
              "IPY_MODEL_6953c898a5874a5c8e810aca25378ea0",
              "IPY_MODEL_b02cb5698e7b427392af57d5c1eed004"
            ],
            "layout": "IPY_MODEL_eb6fcfcc9d1342d68e2596086a47e370"
          }
        },
        "8e7105458491467188f82d4d36d5fa5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0be0f89a2da46c5af2fc5a11eacba7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a581e6c45e784d86b4230c5616de9561": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa08798075ff4f27a6a44180ea253abb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b02cb5698e7b427392af57d5c1eed004": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61f9c7b08fa245dea5ca06a778e7e5b5",
            "placeholder": "​",
            "style": "IPY_MODEL_03373b86e4184a728befbef79a0d9e56",
            "value": " 50/50 [00:00&lt;00:00, 436.74 examples/s]"
          }
        },
        "eb6fcfcc9d1342d68e2596086a47e370": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecc7d7ec8b65459692904d26859d0a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa08798075ff4f27a6a44180ea253abb",
            "placeholder": "​",
            "style": "IPY_MODEL_58cb8a3d4a41402b91fa2d67e80288d3",
            "value": "Map: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
